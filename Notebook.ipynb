{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices: Advanced Regression Techniques\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"images/housesbanner.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import necessary dependencies\n",
    "-----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accessing and Reading Data Sets\n",
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we already downloaded the data and stored it in the`data` directory.\n",
    "To load the two CSV (Comma Separated Values) files containing training and test data respectively we use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "test_data = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s take a look at the first 4 and last 2 features as well as the label (SalePrice) from the first 4 examples:\n",
    "train_data.iloc[0:4, [0,1,2,3,-3,-2,-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that in each example, the first feature is the ID.\n",
    "# This helps the model identify each training example. While this is convenient, it doesn't carry any information for prediction purposes.\n",
    "# Hence we remove it from the dataset before feeding the data into the network.\n",
    "all_features = pd.concat((train_data.iloc[:,1:-1], test_data.iloc[:,1:]))\n",
    "print(all_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we feed it into a deep network, we need toperform some amount of processing.\n",
    "Let’s start with the numerical features. We begin by replacing missing values with the mean.\n",
    "This is a reasonable strategy if features are missing at random.\n",
    "To adjust them to acommon scale, we rescale them to zero mean and unit variance.\n",
    "\n",
    "$$ x \\leftarrow \\frac{x - \\mu}{\\sigma} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The reason for ‘normalizing’ the data is that it brings all features to the same order of magnitude\n",
    "numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index\n",
    "all_features[numeric_features] = all_features[numeric_features].apply(\n",
    "    lambda x: (x-x.mean())/(x.std()))\n",
    "\n",
    "# After standardizing the data all means vanish, hence we can set missing values to 0\n",
    "all_features[numeric_features] = all_features[numeric_features].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we deal with discrete values. This includes variables such as 'MSZoning'. We replace them by a one-hot encoding in the same manner as how we transformed multiclass classification data into a vector of $0$ and $1$. For instance, 'MSZoning' assumes the values 'RL' and 'RM'. They map into vectors $(1,0)$ and $(0,1)$ respectively. Pandas does this automatically for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy_na=True refers to a missing value being a legal eigenvalue, and\n",
    "# creates an indicative feature for it\n",
    "all_features = pd.get_dummies(all_features, dummy_na=True)\n",
    "all_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that this conversion increases the number of features from 79 to 331.\n",
    "\n",
    "Finally, via the `from_numpy` attribute, we can extract the NumPy format from the Pandas dataframe and convert it into PyTorch’s native Tensor representation for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = train_data.shape[0]\n",
    "num_features = all_features.shape[1]\n",
    "train_features = torch.from_numpy(all_features[:num_train].values).float()\n",
    "test_features = torch.from_numpy(all_features[num_train:].values).float()\n",
    "train_labels = torch.from_numpy(train_data.SalePrice.values).float().view(-1, 1)\n",
    "\n",
    "print(f'num_features: {num_features}')\n",
    "print(f'num_train: {num_train}')\n",
    "print(f'train_features: {train_features.size()}')\n",
    "print(f'train_labels: {train_labels.size()}')\n",
    "print(f'test_features: {test_features.size()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HousePriceDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, train_features, train_labels, transforms=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_features (tensor): PyTorch Tensor with shape of (num_examples, features).\n",
    "            train_labels (tensor): PyTorch Tensor with shape of (num_examples, 1).\n",
    "            transforms (callable, optional): Optional transforms to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.train_features = train_features\n",
    "        self.train_labels = train_labels\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.train_features.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        features = self.train_features[idx]\n",
    "        label = self.train_labels[idx]\n",
    "\n",
    "        if self.transforms:\n",
    "            features = self.transforms(features)\n",
    "\n",
    "        return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(num_features, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With house prices, as with stock prices, we care about relative quantities more than absolute quantities.\n",
    "More concretely, we tend to care more about the relative error $\\frac{y - \\hat{y}}{y}$ than about the absolute error $y - \\hat{y}$.\n",
    "For instance, if our prediction is off by USD 100,000 when estimating the price of a house in Rural Ohio, where the value of a typical house is 125,000 USD, then we are probably doing a horrible job.\n",
    "On the other hand, if we err by this amount in Los Altos Hills, California, this might represent a stunningly accurate prediction (their, the median house price exceeds 4 million USD).\n",
    "\n",
    "One way to address this problem is to measure the discrepancy in the logarithm of the price estimates. In fact, this is also the official error metric used by the compeitition to measure the quality of submissions. After all, a small value $\\delta$ of $\\log y - \\log \\hat{y}$ translates into $e^{-\\delta} \\leq \\frac{\\hat{y}}{y} \\leq e^\\delta$. This leads to the following loss function:\n",
    "$$L = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\log y_i -\\log \\hat{y}_i\\right)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_rmse(net, features, labels):\n",
    "    # To further stabilize the value when the logarithm is taken, set the\n",
    "    # value less than 1 as 1\n",
    "    clipped_preds = torch.clamp(net(features), min=1, max=float('inf'))\n",
    "    rmse = torch.sqrt(criterion(clipped_preds.log(), labels.log()))\n",
    "    return rmse.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike in previous sections, our training functions here will rely on the Adam optimizer (a slight variant on SGD that we will describe in greater detail later).\n",
    "The main appeal of Adam vs vanilla SGD is that the Adam optimizer, despite doing no better (and sometimes worse) given unlimited resources for  hyperparameteroptimization, people tend to find that it is significantly less sensitive to the initial learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if train_on_gpu else 'cpu')\n",
    "\n",
    "# Model allocation\n",
    "net.to(device)\n",
    "\n",
    "train_ls, test_ls = [], []\n",
    "\n",
    "def train(net, train_features, train_labels, test_features, test_labels,\n",
    "          num_epochs, learning_rate, weight_decay, batch_size):\n",
    "#     train_ls, test_ls = [], []\n",
    "    \n",
    "    trainset = HousePriceDataset(train_features, train_labels)\n",
    "    trainloader = DataLoader(trainset, batch_size=batch_size,\n",
    "                             shuffle=True, num_workers=4)\n",
    "    \n",
    "    train_features = train_features.to(device)\n",
    "    train_labels = train_labels.to(device)\n",
    "    \n",
    "    # The Adam optimization algorithm is used here\n",
    "    optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Iterate over data.\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for i, batch in enumerate(trainloader):\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            labels = labels.view(-1, 1)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # statistics\n",
    "        epoch_loss = log_rmse(net, train_features, train_labels)\n",
    "        train_ls.append(epoch_loss)\n",
    "        print(f'Loss: {epoch_loss}')\n",
    "        \n",
    "        if test_labels is not None:\n",
    "            test_ls.append(log_rmse(net, test_features, test_labels))\n",
    "    \n",
    "    return net, train_ls, test_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorBoard setup\n",
    "# writer = SummaryWriter('runs')\n",
    "EPOCHS = 100\n",
    "\n",
    "net = train(net, train_features, train_labels, test_features, test_labels=None,\n",
    "            num_epochs=EPOCHS, learning_rate=0.001, weight_decay=None, batch_size=32)\n",
    "\n",
    "# writer.close()\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel(\"Loss Magnitude\")\n",
    "plt.plot([x for x in range(EPOCHS)], train_ls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-Fold Cross-Validation\n",
    "\n",
    "If you are reading in a linear fashion, you might recall that we intorduced k-fold cross-validation in the section where we discussed how to deal with model section (`chapter_model_selection`). We will put this to good use to select the model design and to adjust the hyperparameters. We first need a function that returns the i-th fold of the data in a k-fold cross-validation procedure. It proceeds by slicing out the i-th segment as validation data and returning the rest as training data. Note that this is not the most efficient way of handling data and we would definitely do something much smarter if our dataset was considerably larger. But this added complexity might obfuscate our code unnecessarily so we can safely omit here owing to the simplicity of our problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_k_fold_data(k, i, X, y):\n",
    "    assert k > 1\n",
    "    fold_size = X.shape[0] // k\n",
    "    X_train, y_train = None, None\n",
    "    for j in range(k):\n",
    "        idx = slice(j * fold_size, (j + 1) * fold_size)\n",
    "        X_part, y_part = X[idx, :], y[idx]\n",
    "        if j == i:\n",
    "            X_valid, y_valid = X_part, y_part\n",
    "        elif X_train is None:\n",
    "            X_train, y_train = X_part, y_part\n",
    "        else:\n",
    "            X_train = nd.concat(X_train, X_part, dim=0)\n",
    "            y_train = nd.concat(y_train, y_part, dim=0)\n",
    "    return X_train, y_train, X_valid, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training and verification error averages are returned when we train $k$ times in the k-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(k, X_train, y_train, num_epochs,\n",
    "           learning_rate, weight_decay, batch_size):\n",
    "    train_l_sum, valid_l_sum = 0, 0\n",
    "    for i in range(k):\n",
    "        data = get_k_fold_data(k, i, X_train, y_train)\n",
    "        net = get_net()\n",
    "        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,\n",
    "                                   weight_decay, batch_size)\n",
    "        train_l_sum += train_ls[-1]\n",
    "        valid_l_sum += valid_ls[-1]\n",
    "        if i == 0:\n",
    "            d2l.plot(list(range(1, num_epochs+1)), [train_ls, valid_ls],\n",
    "                     xlabel='epoch', ylabel='rmse', \n",
    "                     legend=['train', 'valid'], yscale='log')\n",
    "        print('fold %d, train rmse: %f, valid rmse: %f' % (\n",
    "            i, train_ls[-1], valid_ls[-1]))\n",
    "    return train_l_sum / k, valid_l_sum / k\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we pick an un-tuned set of hyperparameters and leave it up to the reader to improve the model. Finding a good choice can take quite some time, depending on how many things one wants to optimize over. Within reason, the k-fold cross-validation approach is resilient against multiple testing. However, if we were to try out an unreasonably large number of options it might fail since we might just get lucky on the validation split with a particular set of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64\n",
    "train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,\n",
    "                          weight_decay, batch_size)\n",
    "print('%d-fold validation: avg train rmse: %f, avg valid rmse: %f'\n",
    "      % (k, train_l, valid_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "----------\n",
    "\n",
    "- Real data often contains a mix of different datatypes and needs to be preprocessed.\n",
    "- Rescaling real-valued data to zero mean and unit variance is a good default. So is replacing missing values with their mean.\n",
    "- Transforming categorical variables into indicator variables allows us to treat them like vectors.\n",
    "- We can use k-fold cross validation to select the model and adjust the hyper-parameters.\n",
    "- Logarithms are useful for relative loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "------------\n",
    "\n",
    "1. Submit your predictions for this tutorial to Kaggle. How good are your predictions?\n",
    "2. Can you improve your model by minimizing the log-price directly? What happens if you try to predictthe log price rather than the price?\n",
    "3. Is it always a good idea to replace missing values by their mean? Hint - can you construct a situationwhere the values are not missing at random?\n",
    "4. Find a better representation to deal with missing values. Hint - What happens if you add an indicatorvariable?\n",
    "5. Improve the score on Kaggle by tuning the hyperparameters through k-fold cross-validation.\n",
    "6. Improve the score by improving the model (layers, regularization, dropout).\n",
    "7. What happens if we do not standardize the continuous numerical features like we have done in thissection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
